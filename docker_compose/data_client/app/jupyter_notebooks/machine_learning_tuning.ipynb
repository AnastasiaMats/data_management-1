{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы узнали про достаточное количество алгоритмов машинного обучения и про то, как оценивать качество алгоритмов\n",
    "\n",
    "Следующий шаг - научиться настраивать алгоритмы для получения максимального качества\n",
    "\n",
    "\n",
    "# Feature engeneering\n",
    "\n",
    "В задачи unsupervised и supervised объединяет общий элемент - матрица *объекты* $\\times$ *признаки* размерность $m \\times n$, где $m$ - число объектов, а $n$ - число признаков\n",
    "\n",
    "$$\n",
    "X = \\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\ldots & x_{14} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{14} \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "x_{m1} & x_{m2} & \\ldots & x_{mn} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Таким образом, каждый объект описан признаками(фичами) в количестве $n$ штук: $x^i = (x_1, x_2, \\ldots, , x_n)$. Мы уже знаем, что фичи бывают численными и категориальными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фичи можно назвать \"топливом\" алгоритмов машинного обучения. Хорошие фичи позволят повысить качество решения задачи, примерно как на картинке\n",
    "\n",
    "![ml_blackbox](img/ml_blackbox.png)\n",
    "\n",
    "\n",
    "В алгоритмах машинного обучения и анализа данных часто встречаются требования к фичам входных данных\n",
    "* распределение данных\n",
    "* масштаб\n",
    "\n",
    "Перед аналитиком часто стоит задача трансформации (преобразования) входных данных таким образом, чтобы удовлетворить условиям алгоритма. Игнорирование требований к входным данным приводит некорректным выводам, это омновной принцип ML (и не только ML): **garbage in - garbage out**. Процесс \"придумывания\" фичей называется feature engineering\n",
    "\n",
    "В этом занятии поговорим о том, как трансформировать исходный csv файл в набор фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Непрерывные переменные\n",
    "\n",
    "В задачах линейной регрессии такие трансформации особенно важны - чтобы линейная регрессия хорошо работала должны выполняться следующие требования:\n",
    "\n",
    "* остатки регрессии должны иметь нормальное (гауссово) распределение\n",
    "* все фичи должны быть примерно в одном масшабе\n",
    "\n",
    "При подготовке данных для обучения линейной регрессии применяются следющие приемы: масштабирование и нормализация.\n",
    "\n",
    "### Монотонные преобразования\n",
    "\n",
    "Существуют чисто инженерные приёмы первичной обработки данных, например для борьбы с большими по модулю значениями обычно используют\n",
    "\n",
    "* логарифмирование np.log\n",
    "* извлечение квадратного корня np.sqrt\n",
    "\n",
    "Оба этих преобразования являются *монотонными*, т.е. они меняют абсолютные значения, но сохраняют порядок величин.\n",
    "\n",
    "### z-score\n",
    "\n",
    "Более интересный метод - это Standart Scaling или Z-score normalization. Это преобразование позволяет \"сгладить\" данные, избавить их от выбросов. Для этого есть инструмента [есть реализация в sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Код                                               Тема  \\\n",
      "0  HYDRA-535  Пробрасывать пользовательское распределение pa...   \n",
      "1  HYDRA-534   Гибридный рекомендатель с multi-channel feedback   \n",
      "2  HYDRA-532         Джоба в дженкинсе для расчёта динамики РВП   \n",
      "\n",
      "       Компонент  Затрачено в часах  \n",
      "0        echidna                  1  \n",
      "1          hydra                  3  \n",
      "2  hydramatrices                  2  \n",
      "Сырой датасет: [ 1.  3.  2.  4.  2. 10.  2.  5.  2.  2.  1.  7.  5.  2.  5. 16. 10.  3.\n",
      " 24.]\n",
      "stat = 0.7262855768203735, p-value=0.00011633868416538462\n",
      "\n",
      "z-transform датасет: [-0.79860216 -0.4497874  -0.6241948  -0.27538007 -0.6241948   0.7710641\n",
      " -0.6241948  -0.10097268 -0.6241948  -0.6241948  -0.79860216  0.24784204\n",
      " -0.10097268 -0.6241948  -0.10097268  1.8175083   0.7710641  -0.4497874\n",
      "  3.2127674 ]\n",
      "stat = 0.7262856960296631, p-value=0.00011633900430751964\n",
      "\n",
      "Проверка на нормальность p_1 > p_2: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "data = pd.read_csv('data/task.csv')\n",
    "print(data.head(3))\n",
    "\n",
    "raw_data = data[data.columns.values[-1]].values.astype(np.float32)\n",
    "print(\"Сырой датасет: %s\" % raw_data)\n",
    "print(\"stat = %s, p-value=%s\\n\" % shapiro(raw_data) )\n",
    "\n",
    "transformed_data = StandardScaler().fit_transform(raw_data.reshape(-1, 1)).reshape(-1)\n",
    "print(\"z-transform датасет: %s\" % transformed_data)\n",
    "print(\"stat = %s, p-value=%s\\n\" % shapiro(transformed_data) )\n",
    "\n",
    "print(\"Проверка на нормальность p_1 > p_2: %s\" % (shapiro(transformed_data)[1] > shapiro(raw_data)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест Шапиро-Уилка показывает, что гипотеза о нормальном распределении данных стала чуть более вероятной, чем до \"Z-score\" нормализации.\n",
    "\n",
    "### min-max normalization\n",
    "\n",
    "Другой распространённый метод называется MinMax Scaling. Этот метод переносит все точки на отрезок [0-1]\n",
    "$$\n",
    "X_{scaled} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сырой датасет: [ 1.  3.  2.  4.  2. 10.  2.  5.  2.  2.  1.  7.  5.  2.  5. 16. 10.  3.\n",
      " 24.]\n",
      "Min-Max scale датасет: [0.         0.08695652 0.04347826 0.13043478 0.04347826 0.39130437\n",
      " 0.04347826 0.17391305 0.04347826 0.04347826 0.         0.2608696\n",
      " 0.17391305 0.04347826 0.17391305 0.65217394 0.39130437 0.08695652\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "print(\"Сырой датасет: %s\" % raw_data)\n",
    "\n",
    "transformed_data = MinMaxScaler().fit_transform(raw_data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "print(\"Min-Max scale датасет: %s\" % transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Категориальные переменные\n",
    "\n",
    "Категориальная переменная - это набор меток (классов). В приложенном затасете по задам столбец `Компонент` - категориальная фича, а `Затрачено в часах` - непрерывная\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Код</th>\n",
       "      <th>Тема</th>\n",
       "      <th>Компонент</th>\n",
       "      <th>Затрачено в часах</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HYDRA-535</td>\n",
       "      <td>Пробрасывать пользовательское распределение pa...</td>\n",
       "      <td>echidna</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HYDRA-534</td>\n",
       "      <td>Гибридный рекомендатель с multi-channel feedback</td>\n",
       "      <td>hydra</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HYDRA-532</td>\n",
       "      <td>Джоба в дженкинсе для расчёта динамики РВП</td>\n",
       "      <td>hydramatrices</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYDRA-531</td>\n",
       "      <td>Интеграция Hydra с Gamora</td>\n",
       "      <td>hydramatrices</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HYDRA-530</td>\n",
       "      <td>Тестируем интеграцию с Jira</td>\n",
       "      <td>hydra</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Код                                               Тема  \\\n",
       "0  HYDRA-535  Пробрасывать пользовательское распределение pa...   \n",
       "1  HYDRA-534   Гибридный рекомендатель с multi-channel feedback   \n",
       "2  HYDRA-532         Джоба в дженкинсе для расчёта динамики РВП   \n",
       "3  HYDRA-531                          Интеграция Hydra с Gamora   \n",
       "4  HYDRA-530                        Тестируем интеграцию с Jira   \n",
       "\n",
       "       Компонент  Затрачено в часах  \n",
       "0        echidna                  1  \n",
       "1          hydra                  3  \n",
       "2  hydramatrices                  2  \n",
       "3  hydramatrices                  4  \n",
       "4          hydra                  2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/task.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество различных меток в поле \"Компонент\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra            11\n",
       "hydramatrices     6\n",
       "echidna           1\n",
       "hydra_utils       1\n",
       "Name: Компонент, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Компонент'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование One-Hot\n",
    "\n",
    "Кодируем вектор, где все нули и одна единица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded = ohe.fit_transform(df[['Компонент']])\n",
    "\n",
    "one_hot_encoded.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing trick\n",
    "\n",
    "В случае, когда признаков слишком много, применяют хеширование\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echidna -> 1\n",
      "hydra -> 2\n",
      "hydramatrices -> 9\n",
      "hydra_utils -> 6\n"
     ]
    }
   ],
   "source": [
    "for label in df['Компонент'].unique():\n",
    "    #print(label, '->', hash(label) % 8 )\n",
    "    print(label, '->', hash(label) % 12 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Премер хеширования (с формулами!) в [Лекциях от ВШЭ](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture06-linclass.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прекрасный разбор есть на хабре в статье от [ODS про feature engineering](https://habr.com/ru/company/ods/blog/326418/#rabota-s-kategorialnymi-priznakami-label-encoding-one-hot-encoding-hashing-trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тексты\n",
    "\n",
    "\n",
    "Bpdktrкак обработка естественного языка (англ. Natural Language Processing, NLP). NLP изучает проблемы компьютерного анализа естественных языков - т.е. языков, которые для общения используют люди (а не придуманных искусственно (например, азбука Морзе - язык, придуманный искусственно). Подробнее о том, зачем нужен NLP и где именно возникает задача обработки естественного языка мы поговорим в Уроке 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты - один из самых доступных и объёмных источников данных. Современный человек потребляет огромное количество контента: фото, звук и текст. Через визуальный канал, т.е. с помощью  фото  и видео, мы получаем 80% всей информации, поэтому такой большой объём курса уделён компьютерному зрению. На втором месте по объёму передаваемой информации - текстовые данные.\n",
    "\n",
    "Например, если у вас интернет-магазин, то для анализа доступны\n",
    "\n",
    "* текстовые описания товаров\n",
    "* пользовательские комментарии\n",
    "* диалоги с продавцом-консультантом в чатике\n",
    "\n",
    "Текстовую информацию просто хранить, поэтому проекты накапливают огромные наборы данных такого рода и очень хотят извлекать из этих объёмов полезную информацию.\n",
    "\n",
    "Как специалист по ML в начале карьеры вы, скорее всего, встретите ряд “классических” задач - например, определение тональности (настроения) текста или классификации сообщений spam/not spam - для таких задач используются подходы, основанные на подсчёте статистик по встречающимся в тексте словам.\n",
    "Однако, есть и другие, более сложные задачи. \n",
    "\n",
    "Для решения применяются различные архитектуры нейросетей (RNN, LSTM) - это мощные инструменты, которые позволяют решать сложные задачи, например: \n",
    "\n",
    "* извлечения именованных сущностей ([NER](https://habr.com/ru/post/414175/), Named-Entity Recognizing)\n",
    "* автоматизированного перевода (например, сервис *google translate* производит перевод с помощью глубоких сетей)\n",
    "* Speech Recognition - распознавание речи, трансляция из аудио в текстовый вид\n",
    "* Natural Language Generation - генерация текстов, например можно генерировать подписи к картинкам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У обработки естественного языка есть ряд особенностей:\n",
    "\n",
    "* необходимо размечать большой объём данных для обучения с учителем. Допустим, хотим отделять спам-сообщения от не спама. Вам нужно найти людей, которые прочитают все смс, которые удалось собрать и отметят те из них, которые являются спамом - текстов обычно очень много и разметка данных может оказаться дорогим удовольствием\n",
    "* модель, обученную на одном языке невозможно использовать для другого языка\n",
    "* важен как синтаксис, так и семантика (смысл). Например, во фразе: «Вот списки студентов, которые сдали зачет по физике» определение «которые сдали зачет по физике» относится к студентам, а в предложении: «Вот списки студентов, которые лежали в шкафу у декана»  структура фразы (тот самый синтаксис) такая же, как и предыдущей - но определение уже относится не к студентам, а к листкам бумаги. От компьютера мы хотим добиться, чтобы смыл обеих фраз был “понят” одинаково хорошо.\n",
    "\n",
    "Кроме того, для текстов на естественном языке довольно сложно проводить предобработку данных, этот этап сильно зависит от задачи, которую вы  решаете. Так, например, для задачи анализа тональности текста знаки препинания, скорее всего, не важны. Однако, для задачи извлечения именованных сущностей (именованная сущность - это имя собственное - например название организации или географического объекта) удалять знаки препинания не рекомендуется - это может привести к потере важной информации. Например если из фразы `Мы пошли обедать в “Берёзку”` если удалить все знаки препинания (кавычки) и заглавную букву в названии заведения то станет сложнее понять, что речь идёт о кафе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста складывается из двух этапов\n",
    "\n",
    "* предварительная обработка текста\n",
    "* векторизация текста\n",
    "\n",
    "#### Предварительная обработка текста\n",
    "\n",
    "Перед тем, как обучать модель, данные следует специальным образом подготовить. Подготовка данных включает в себя несколько обязательных этапов\n",
    "\n",
    "* удалить все нерелевантные символы (например, любые символы, не относящиеся к цифро-буквенным).\n",
    "* токенизировать текст, разделив его на индивидуальные слова (токены)\n",
    "* удалить нерелевантные слова — например, упоминания в Twitter или URL-ы.\n",
    "* перевести все символы в нижний регистр для того, чтобы слова «hello», «Hello» и «HELLO» были схлопнуты в один токен\n",
    "* исправление ошибок (\"молоко\" и \"молако\" - одно слово, но разные токены, не надо так)  \n",
    "* лемматизация - перевод слова в нормальную (словарную) форму (например, «машина» вместо «машиной»). Существительные должны быть приведены к единственному числу именительного падежа, глаголы - инфинитив и т.д.\n",
    "* стемминг - процедура, когда от слова переходим к его корню (\"помыть\" и \"мытый\" - корень \"мыт\"). То есть все \"помытые\" заменяем на \"мыт\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти приёмы нужно применять с осторожностью и внимательно следить за тем, как тот или иной приём, применённый к исходному тексту, влияет на качество решения задачи (например, выявлению спама)\n",
    "\n",
    "Для демонстрации всех этих приёмов загрузим корпус (набор текстов) с твитами о продуктах. Для каждого твита размечена эмоциональная окраска - позитивная, нейтральная или негативная. Примечание: для  обработки текста воспользуемся библиотекой nltk, которая [доступна в anaconda](https://anaconda.org/anaconda/nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3904, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# дополнительный словарь со знаками пунктуации\n",
    "nltk.download('punkt', download_dir='.')\n",
    "\n",
    "df = pd.read_csv('data/brand_tweets.csv', sep=',', encoding='utf8')\n",
    "# удаляем строки, в которых отсутствует текст твита\n",
    "df.drop(df[df.tweet_text.isnull()].index, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала разделим текст на отдельные слова с помощью библиотечной функции word_tokenize, на примере первого документа в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Исходный текст== \n",
      ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n",
      "\n",
      "\n",
      "== Токенизированный текст==\n",
      "['.', '@', 'wesley83', 'I', 'have', 'a', '3G', 'iPhone', '.', 'After', '3', 'hrs', 'tweeting', 'at', '#', 'RISE_Austin', ',', 'it', 'was', 'dead', '!', 'I', 'need', 'to', 'upgrade', '.', 'Plugin', 'stations', 'at', '#', 'SXSW', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_str = df.tweet_text.values[0]\n",
    "\n",
    "print('== Исходный текст== \\n%s\\n\\n' % sample_str)\n",
    "\n",
    "tokenized_str = nltk.word_tokenize(sample_str)\n",
    "print('== Токенизированный текст==\\n%s' % tokenized_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтруем знаки пунктуации, токены приведём к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wesley83', 'i', 'have', 'a', '3g', 'iphone', 'after', '3', 'hrs', 'tweeting', 'at', 'rise_austin', 'it', 'was', 'dead', 'i', 'need', 'to', 'upgrade', 'plugin', 'stations', 'at', 'sxsw']\n"
     ]
    }
   ],
   "source": [
    "tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем стоп-слова, список которых для русского языка можно получить как `stop_words = nltk.corpus.stopwords.words('russian')`. Стоп-слова это \"мусорные\" слова которые встречаются чрезычайно часто (в каждом предложении) поэтому не несут в себе никакой информации. Такие слова, вобщем-то, нужны только для красивой речи и поэтому можем их смело удалять из текста. Например, этот список стоп-слов я нагуглил в интернете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wesley83', '3g', 'iphone', '3', 'hrs', 'tweeting', 'rise_austin', 'dead', 'need', 'upgrade', 'plugin', 'stations', 'sxsw']\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'shold',\n",
    "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
    "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Домашнее задание\n",
    "\n",
    "Реализуем пайплайн в виде функции, при помощи которой обработаем все текстовые описания. Для каждого описания\n",
    "* проводим токенизацию\n",
    "* удаляем пунктуацию\n",
    "* приводим к нижнему регистру\n",
    "* удаляем стоп-слова\n",
    "\n",
    "\n",
    "Примените процедуру токенизации к файлу brand_tweets_valid.csv\n",
    "\n",
    "Сколько уникальных токенов получилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    []\n",
       "1    []\n",
       "2    []\n",
       "3    []\n",
       "4    []\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(raw_text: str):\n",
    "    \"\"\"Функция для токенизации текста\n",
    "    \n",
    "    :param raw_text: исходная текстовая строка\n",
    "    \"\"\"\n",
    "    filtered_tokens = []\n",
    "    # -- ВАШ КОД ТУТ --\n",
    "    \n",
    "    # -----------------\n",
    "    return filtered_tokens\n",
    "\n",
    "# применяем функцию в датафрейму с помощью метода .apply()\n",
    "tokenized_tweets= df.tweet_text.apply(tokenize_text)\n",
    "\n",
    "# добавляем новую колонку в исходный датафрейм\n",
    "df = df.assign(\n",
    "    tokenized=tokenized_tweets\n",
    ")\n",
    "\n",
    "df.tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - привести слово в нормальную (словарную) форму. Для русского языка мы уже проводили нормализацию (в занятии из цикла по Аналитике начального уровня) с помощью модуля pyMorphy, который отлично подходит для русского языка\n",
    "\n",
    "<pre>\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "parsed_token = morph.parse(word)\n",
    "normal_form = parsed_token[0].normal_form\n",
    "</pre>\n",
    "\n",
    "В силу того, что наши твиты на английском языке, то этап нормализации не слишком актуален.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста: Bag of Words\n",
    "\n",
    "\n",
    "Мы умеем подготавливать текст к обработке: приводить слова к начальным формам, разделять текст на токены, удалять \"мусорные\" токены (стоп-слова). Однако, мы знаем, что нейросети работают не с текстом, а с числами. Давайте разбираться, как переводить токены в числа, то есть с тем, как работает векторизация\n",
    "\n",
    "Bag of Words - это способ перейти от набора токенов к численному вектору. Алгоритм векторизации текста по модели BoW:\n",
    "\n",
    "1. определяем количество $N$ различных токенов во всех доступных текста - так называемый \"словарь\"\n",
    "1. присваиваем каждому токену случайный номер от $0$ до $N$\n",
    "1. для каждого документа $i$ формируем вектор размерности $N$ - ставим на позицию $j$ количество вхождений токена с номером $j$, которые содержатся в тексте $i$.\n",
    "\n",
    "Каждый токен мы по сути представляем в виде вектора размерности $N$, который состоит из нулей и всего одной единицы, такое кодирование называется *One-Hot encoding*. А каждый документ это \"сумма\" всех one-hot векторов входящих в него токенов\n",
    "\n",
    "Такой подход хорошо иллюстрируется картинкой:\n",
    "\n",
    "![bow](img/bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого элемента получаем вектор из нулей и единиц. При этом размерность словаря обычно составляет несколько десятков тысяч, а количество токенов в одном документе несколько десятков - то есть нулей значительно больше, чем единиц - такие данные называются *разреженными*.\n",
    "\n",
    "В таком виде данные уже пригодны для работы с нейросетью или любым другим алгоритмом ML, однако есть несколько довольно простых и полезных вещей, которые мы можем сделать и без нейросетей. Давайте сначала разберем их, а потом вернемся к нейросетям. Такое представление текста позволяет решать интересные задачи - например, находить самые похожие друг на друга тексты. Чтобы как-то формализовать понятие \"схожести\" текстов, вводится понятие *косинусного расстояния* между двумя векторами текстов $a$ и $b$ размерности $N$. С этой метрикой вы [можете познакомиться в Википедии](https://ru.wikipedia.org/wiki/Векторная_модель#Косинусное_сходство ), формула такая для двух векторов $a$ и $b$ с координатами $a_i$ и $b_i$ соответственно:\n",
    "$$\n",
    "\\text{similarity} = \\cos (\\theta) = 1 - \\frac{\\sum_{i=1}^{N}a_ib_i}{\\sqrt{\\sum_{i=1}^{N}(a_i)^2}\\sqrt{\\sum_{i=1}^{N}(b_i)^2}}\n",
    "$$\n",
    "\n",
    "Интуитивное объяснение для простого случая: два документа полностью совпадают, тогда единички в них стоят на одних и тех же местах - расстояние между ними будет нулевым. Если два текста совершенно не пересекаются, то единички будут стоять на разных местах - расстояние в этом случае равно единице. Самостоятельно реализовывать функцию не нужно - есть готовая реализация в [scipy.spatial.distance.cosine](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html)\n",
    "\n",
    "Векторизуем наш корпус (набор текстов) с помощью класса `CountVectorizer()` (то есть превращаем наборы токенов в наборы векторов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7a66e096b05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# применяем наш объект-токенизатор к датафрейму с твитами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdocument_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# результат - матрица, в которой находятся числа, строк в мастрице столько, сколько документов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# а столбцов столько, сколько токенов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/www/app/.venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/www/app/.venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# инициализируем объект, который токенизирует наш текст\n",
    "# в качестве единственного аргимента передаём функцию, которую мы написали в Уроке 2\n",
    "# на разбивает каждый документ на токены\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_text)\n",
    "# применяем наш объект-токенизатор к датафрейму с твитами\n",
    "document_matrix = vectorizer.fit_transform(df.tweet_text.values)\n",
    "# результат - матрица, в которой находятся числа, строк в мастрице столько, сколько документов\n",
    "# а столбцов столько, сколько токенов\n",
    "document_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс `sklearn.feature_extraction.text.CountVectorizer` реализует алгоритм преобразования массива текстовых документов в разреженную матрицу такую, что\n",
    "\n",
    "* число строк совпадает с количеством документов в исходном датафрейме\n",
    "* количество столбцов совпадает с количеством различных токенов\n",
    "* объект `CountVectorizer()` содержит в себе разные вспомогательные элементы - например, словарь соответствия токена и его номера\n",
    "\n",
    "Полученные вектора можно использовать в алгоритмах второго уровня - например, в задаче классификации отзывов.\n",
    "\n",
    "Пользуясь матрицей, найдем твит, который максимально похож на первый твит из набора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n"
     ]
    }
   ],
   "source": [
    "source_tweet_index = 0\n",
    "print(df.iloc[source_tweet_index].tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем попарные схожести между элементами разреженной матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cceab3d95c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtweet_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "tweet_distance = 1-pairwise_distances(document_matrix, metric=\"cosine\")\n",
    "\n",
    "tweet_distance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем  корпусе  (наборе текстов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be482df992df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# отсортируем твиты по “похожести” - чем похожее на source_tweet_index,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# тем ближе к началу списка sorted_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msorted_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtweet_distance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_tweet_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msorted_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet_distance' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# отсортируем твиты по “похожести” - чем похожее на source_tweet_index,\n",
    "# тем ближе к началу списка sorted_similarity\n",
    "sorted_similarity = np.argsort(-tweet_distance[source_tweet_index,:])\n",
    "\n",
    "sorted_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили вектор \"схожестей\", который содержит индексы похожих твитов, расположенных по убыванию схожести. Больше всего твит похож сам на себя, поэтому возьмём индекс второго по схожести элемента (и далее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n",
      "-------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sorted_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-756ed6028c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[1]].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[2]].tweet_text)\n",
    "print('-------------')\n",
    "print(df.iloc[sorted_similarity[3]].tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили мощный инструмент для анализа текстов - например, мы случайно нашли дубликат твита\n",
    "\n",
    "Кроме простого подхода, когда мы вычисляем счётчик вхождения токена, можно вычислять более сложную метрику TF-IDF (term frequency - inverse document frequency), которая вычисляется по следующей формуле для токена $t$ и документа $d$:\n",
    "$$\n",
    "\\text{tf-idf}(t,d) = \\text{tf}(t,d)\\cdot\\text{idf}(t)\n",
    "$$\n",
    "\n",
    "Где $\\text{tf}(t,d)$ - элемент матрицы, полученной из `CountVectorizer()`, который мы умножаем на величину $\\text{idf}(t)$. \n",
    "\n",
    "Эта величина показывает количество документов в корпусе  (наборе текстов), в которых был встречен токен $t$:\n",
    "$$\n",
    "\\text{idf}(t) = \\log\\frac{1+N}{1+\\text{df(t)}} + 1\n",
    "$$\n",
    "\n",
    "где $\\text{df}(t)$ - количество документов корпуса, в которых был встречен токен $t$. Таким образом мы понижаем веса у слов, которые встречаются почти во всех документах - такие токены являются неинформативными и мусорными, алгоритм понижает их \"важность\" для анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм TF-IDF лучше подходит для анализа текстов и даёт более высокое качество, но более затратен по вычислениям. Как выбрать между этими алгоритмами?\n",
    "\n",
    "* если токенов менее 10000 используйте TF-IDF\n",
    "* если токенов более 10000 то *попробуйте* использовать TF-IDF, если не получится - возвращайтесь к CountVectorizer\n",
    "\n",
    "**Недостатки BoW подхода** Используя алгоритмы вроде Вag of Words, мы теряем порядок слов в тексте, а значит, тексты \"i have no cows\" и \"no, i have cows\" будут идентичными после векторизации, хотя и противоположными семантически. Чтобы избежать этой проблемы, можно сделать шаг назад и изменить подход к токенизации: например, использовать N-граммы (комбинации из N последовательных токенов). Обычно по корпусу  (набору текстов) формируются биграммы (последовательности из двух слов) или триграммы (последовательности из трёх слов)\n",
    "\n",
    "Кроме того, текст можно разбивать не на слова, а на последовательности букв - при таком подходе опечатки будут автоматически учитываться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Домашнее задание\n",
    "\n",
    "Потренируйтесь в нахождении матрицы схожести для валидационного сета\n",
    "\n",
    "загрузите brand_tweets_valid.csv\n",
    "примените объект vectorizer, обученный на датасете brand_tweets.csv (просто скопируйте этот код из урока)\n",
    "примените функцию pairwise_distances к полученной матрице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_valid = pd.read_csv('data/brand_tweets_valid.csv', sep=',', encoding='utf8')\n",
    "# удаляем строки, в которых отсутствует текст твита\n",
    "df_valid.drop(df[df.tweet_text.isnull()].index, inplace=True)\n",
    "\n",
    "# -- ВАШ КОД ТУТ --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Пользуясь матрицей схожести, полученной на предыдущем этапе, найдите top-5 твитов, похожих на твит валидационного сета с id=14.\n",
    "\n",
    "У вас есть матрица схожести между объектами. Попробуйте решить задачу поиска дубликатов в тексте\n",
    "\n",
    "1. Визуализируйте гистограмму значений в матрице схожести\n",
    "1. Напишите функцию на Python, которая принимает индекс твита, пороговое значение (число от $0.0$ до $1.0$ и матрицу схожести, а затем выводит все твиты, схожесть которых больше, чем пороговое значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
